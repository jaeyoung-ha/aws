## 7. Dev Endpoint 사용

### 7.1. Dev Endpoint 생성

* 효율적인 개발/테스트을 위한 Dev Endpoint를 추가합니다.  

<img width="1071" alt="Screen Shot 2020-05-01 at 12 02 05 PM" src="https://user-images.githubusercontent.com/6407492/80778712-42cf4a00-8ba4-11ea-844f-35129f42bf20.png">

* Dev Endpoint 이름과 IAM role을 입력합니다.  

<img width="1045" alt="Screen Shot 2020-05-01 at 12 03 35 PM" src="https://user-images.githubusercontent.com/6407492/80778715-45ca3a80-8ba4-11ea-987e-c95831ac5580.png">

* Networking 옵션을 설정합니다.  

<img width="1010" alt="Screen Shot 2020-05-01 at 12 05 34 PM" src="https://user-images.githubusercontent.com/6407492/80778717-4793fe00-8ba4-11ea-9f9d-c09a2d64bea0.png">

* SSH public key 설정을 스킵합니다.  

<img width="1014" alt="Screen Shot 2020-05-01 at 12 05 46 PM" src="https://user-images.githubusercontent.com/6407492/80778723-4a8eee80-8ba4-11ea-956c-5152712dd4c8.png">

* 입력한 정보를 확인하고, Dev Endpoint 생성을 완료합니다.  

<img width="1047" alt="Screen Shot 2020-05-01 at 12 05 57 PM" src="https://user-images.githubusercontent.com/6407492/80778729-4c58b200-8ba4-11ea-8ad6-73e76997b329.png">

* Dev Endpoint 생성이 완료되는데, 수분의 시간이 걸립니다. 

<img width="1034" alt="Screen Shot 2020-05-01 at 12 12 11 PM" src="https://user-images.githubusercontent.com/6407492/80778938-0819e180-8ba5-11ea-8c80-2910601ae2c0.png">

### 7.2. Notebook 생성  

* SageMaker Notebook을 생성합니다. 

<img width="1040" alt="Screen Shot 2020-05-01 at 12 13 04 PM" src="https://user-images.githubusercontent.com/6407492/80778967-21bb2900-8ba5-11ea-8540-8ff63fd6e9cd.png">

* Notebook 이름, Dev Endpoint와 IAM role을 설정하고, "Create notebook"을 클릭합니다.  

<img width="616" alt="Screen Shot 2020-05-01 at 12 14 37 PM" src="https://user-images.githubusercontent.com/6407492/80779026-5af39900-8ba5-11ea-9b5c-43a4e9dd6622.png">

* Notebook 생성이 완료되는데, 수분의 시간이 걸립니다. 

<img width="1046" alt="Screen Shot 2020-05-01 at 12 32 22 PM" src="https://user-images.githubusercontent.com/6407492/80779850-d48c8680-8ba7-11ea-8e3c-a179f82bb2c4.png">

### 7.3. Notebook 실행 

* "Open notebook"을 클릭해서 Notebook을 실행합니다.

<img width="1036" alt="Screen Shot 2020-05-01 at 12 34 02 PM" src="https://user-images.githubusercontent.com/6407492/80780004-4e247480-8ba8-11ea-8ceb-c9ae3e6bdaf8.png">

* "Glue Examples"을 클릭합니다.

<img width="1178" alt="Screen Shot 2020-05-01 at 12 34 23 PM" src="https://user-images.githubusercontent.com/6407492/80780077-9cd20e80-8ba8-11ea-9506-207096d5388e.png">

* "Joining, Filtering, and Loading Relational Data with AWS Glue"를 클릭해서 jupyter 노트북 환경으로 들어갑니다.

<img width="1198" alt="Screen Shot 2020-05-01 at 12 34 46 PM" src="https://user-images.githubusercontent.com/6407492/80780082-9f346880-8ba8-11ea-99c5-90e3e4ea75dc.png">

* 이번 실습에 다루지 않지만, 노트북 예제를 따라하면 아래와 같은 작업을 할 수 있습니다.

  * **S3 데이터를 위한 크롤러 생성 및 실행**
  * **데이터 카탈로그를 통해 S3 데이터 조회 (리스트, 카운트, 필터링)**
  * **테이블 간 Join**
  * **Parquet 파일 생성**
  * **Redshift에 데이터 쓰기**  

* SparkSQL을 사용해서 노트북에서 query를 실행합니다.

<img width="1125" alt="Screen Shot 2020-05-01 at 12 43 13 PM" src="https://user-images.githubusercontent.com/6407492/80780298-7e204780-8ba9-11ea-8e1b-8640ca5586f8.png">

```sql
spark.sql("use s3")
spark.sql("show tables").show()
spark.sql("select * from s3.s3emp").show()
```

* Spark Code를 사용해서 노트북에서 데이터를 분석합니다.

<img width="1127" alt="Screen Shot 2020-05-01 at 12 48 43 PM" src="https://user-images.githubusercontent.com/6407492/80780498-1c141200-8baa-11ea-8569-65d257907269.png">

```python
import sys
from awsglue.transforms import *
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
glueContext = GlueContext(SparkContext.getOrCreate())

emp = glueContext.create_dynamic_frame.from_catalog(database="s3", table_name="s3emp")
emp.printSchema()
emp.toDF().show()
```

---
### Table of Contents
[Analytics on AWS](README.md)
1. [Oracle RDS 생성](100.create_oracle_rds.md)
2. [Sample 테이블 생성 및 데이터 추가](200.load_sample_data.md)
3. [Glue를 통해 Oracle 데이터를 S3로 이동](300.migrate_data_through_glue.md)
4. [DMS를 통해 Oracle 데이터를 S3로 이동](400.migrate_data_through_dms.md)
5. [Glue를 통해 S3 데이터에 대한 데이터 카탈로그 구성](500.create_data_catalog.md)
6. [Redshift에서 S3 데이터 조회](600.select_s3_data_in_redshift.md)
7. [Dev Endpoint 사용](700.use_dev_endpoint.md)
