## 3. Glue를 통해 Oracle 데이터를 S3로 이동

### 3.1 사전 작업

* Glue가 RDS에 접근할 수 있도록, RDS의 Security Group에 Inbound Rule을 추가합니다. 

  * 참조 URL : [https://docs.aws.amazon.com/ko_kr/glue/latest/dg/setup-vpc-for-glue-access.html](https://docs.aws.amazon.com/ko_kr/glue/latest/dg/setup-vpc-for-glue-access.html)   

<img width="1090" alt="Screen Shot 2020-04-30 at 12 05 39 PM" src="https://user-images.githubusercontent.com/6407492/80667989-945fd200-8adb-11ea-83f1-4921d3349de4.png">

* Glue가 S3에 접근할 수 있도록, S3에 대한 VPC Endpoint를 추가합니다.  

  * 참조 URL : [https://docs.aws.amazon.com/ko_kr/glue/latest/dg/vpc-endpoints-s3.html](https://docs.aws.amazon.com/ko_kr/glue/latest/dg/vpc-endpoints-s3.html)

<img width="1207" alt="Screen Shot 2020-04-30 at 12 22 23 PM" src="https://user-images.githubusercontent.com/6407492/80668640-59f73480-8add-11ea-9de5-024ffcebc5d1.png">

<img width="1095" alt="Screen Shot 2020-04-30 at 12 22 57 PM" src="https://user-images.githubusercontent.com/6407492/80668654-61b6d900-8add-11ea-8d05-2085bb18c5f4.png">

### 3.2. Connection 생성

* Data catalog 구성을 위한 Connection을 생성합니다.  

<img width="1249" alt="Screen Shot 2020-04-30 at 11 55 31 AM" src="https://user-images.githubusercontent.com/6407492/80667988-945fd200-8adb-11ea-9d42-9b2ef5b7ccb5.png">

* Connection type, Database engine 정보를 입력합니다.  

<img width="998" alt="Screen Shot 2020-04-30 at 12 55 36 PM" src="https://user-images.githubusercontent.com/6407492/80670584-469a9800-8ae2-11ea-9eb0-fa133e570160.png">

* Instance, Database name, Username, Password를 입력합니다.   

<img width="1088" alt="Screen Shot 2020-04-30 at 12 56 19 PM" src="https://user-images.githubusercontent.com/6407492/80670587-48645b80-8ae2-11ea-8325-54adbd340056.png">

* 입력한 정보를 확인하고 Connection 생성을 완료합니다.  

<img width="1038" alt="Screen Shot 2020-04-30 at 12 56 32 PM" src="https://user-images.githubusercontent.com/6407492/80670592-4a2e1f00-8ae2-11ea-8738-0ab329490f46.png">

* Connections list에서 입력한 Connection 정보를 확인합니다.  

<img width="1214" alt="Screen Shot 2020-04-30 at 12 56 48 PM" src="https://user-images.githubusercontent.com/6407492/80670597-4c907900-8ae2-11ea-8d1c-72874a00dfd1.png">

* Connection을 테스트합니다.   

  * 이번 실습에서는 AWSGlueServiceRole과 AmazonS3FullAccess Policy를 **AWSGlueServiceRole-test** Role에 추가하였습니다.
  
  ![Screen Shot 2020-04-30 at 3 28 59 PM](https://user-images.githubusercontent.com/6407492/80679787-8c625b00-8af8-11ea-9364-5c0ff540f4a9.png) 
  
<img width="1216" alt="Screen Shot 2020-04-30 at 12 57 06 PM" src="https://user-images.githubusercontent.com/6407492/80670601-4e5a3c80-8ae2-11ea-87a5-1e20f5fcee13.png">

<img width="1218" alt="Screen Shot 2020-04-30 at 12 57 21 PM" src="https://user-images.githubusercontent.com/6407492/80670603-50240000-8ae2-11ea-9e0d-11f8e77d6567.png">

* 테스트 성공을 확인합니다.  

<img width="1218" alt="Screen Shot 2020-04-30 at 12 57 42 PM" src="https://user-images.githubusercontent.com/6407492/80670608-51edc380-8ae2-11ea-9a0c-cb6334754a80.png">

### 3.3. Crawler 생성 및 실행  

* Database의 Meta data 정보를 가져오기 위한 Crawler를 생성합니다.  

<img width="997" alt="Screen Shot 2020-04-30 at 1 41 56 PM" src="https://user-images.githubusercontent.com/6407492/80673646-c9bfec00-8aea-11ea-857a-2b2c59fb9d10.png">

* Crawler source type을 설정합니다.  

<img width="978" alt="Screen Shot 2020-04-30 at 1 42 09 PM" src="https://user-images.githubusercontent.com/6407492/80673648-cb89af80-8aea-11ea-8917-988391d9a4e5.png">

* Connection을 선택합니다.  

<img width="1003" alt="Screen Shot 2020-04-30 at 1 45 24 PM" src="https://user-images.githubusercontent.com/6407492/80673651-cd537300-8aea-11ea-82a2-fc7eacf6cfa8.png">

* 추가적인 Data store가 없으므로, "No"를 선택합니다.  

<img width="879" alt="Screen Shot 2020-04-30 at 1 45 37 PM" src="https://user-images.githubusercontent.com/6407492/80673653-cf1d3680-8aea-11ea-9a8a-fe3b0b4f6f03.png">

* Data store에 접근하기 위한 IAM role을 설정합니다.

  * 이번 실습에서는 AWSGlueServiceRole과 AmazonS3FullAccess Policy를 **AWSGlueServiceRole-test** Role에 추가하였습니다.
  
  ![Screen Shot 2020-04-30 at 3 28 59 PM](https://user-images.githubusercontent.com/6407492/80679787-8c625b00-8af8-11ea-9364-5c0ff540f4a9.png) 

<img width="996" alt="Screen Shot 2020-04-30 at 1 45 52 PM" src="https://user-images.githubusercontent.com/6407492/80673656-d0e6fa00-8aea-11ea-8e97-1428040b071d.png">

* Crawler를 위한 스케줄 설정을 할 수 있습니다. 이번 실습에서는 "Run on demand"로 설정합니다.  

<img width="1001" alt="Screen Shot 2020-04-30 at 1 46 04 PM" src="https://user-images.githubusercontent.com/6407492/80673660-d2182700-8aea-11ea-9856-0c30a1591968.png">

* Crawler의 결과를 저장할 Database를 설정합니다.  

<img width="1004" alt="Screen Shot 2020-04-30 at 1 46 23 PM" src="https://user-images.githubusercontent.com/6407492/80673663-d3e1ea80-8aea-11ea-8e2d-fb449befb55b.png">

* 입력한 Crawler 정보를 확인하고 Crawler 생성을 완료합니다.  

<img width="1073" alt="Screen Shot 2020-04-30 at 1 46 41 PM" src="https://user-images.githubusercontent.com/6407492/80673674-d6dcdb00-8aea-11ea-9474-58d9ad6272fc.png">

* Crawler를 실행합니다.  

<img width="1211" alt="Screen Shot 2020-04-30 at 1 46 59 PM" src="https://user-images.githubusercontent.com/6407492/80673854-4f439c00-8aeb-11ea-9cec-2450bd0c9744.png">

* Crawler가 작업을 시작합니다. 

<img width="1216" alt="Screen Shot 2020-04-30 at 1 47 17 PM" src="https://user-images.githubusercontent.com/6407492/80673861-510d5f80-8aeb-11ea-946e-4d8d2efb4a00.png">

<img width="1218" alt="Screen Shot 2020-04-30 at 1 49 44 PM" src="https://user-images.githubusercontent.com/6407492/80673865-52d72300-8aeb-11ea-8a52-10aee3706bf4.png">

* Crawler가 작업을 완료합니다.  

<img width="1220" alt="Screen Shot 2020-04-30 at 1 49 59 PM" src="https://user-images.githubusercontent.com/6407492/80673870-54a0e680-8aeb-11ea-8feb-1c384fdc5f16.png">

* Crawler의 작업 완료를 확인합니다.  

<img width="1218" alt="Screen Shot 2020-04-30 at 1 51 15 PM" src="https://user-images.githubusercontent.com/6407492/80673874-566aaa00-8aeb-11ea-8cc5-dfb0f6895e26.png">

* Crawler가 추출한 Meta data 테이블을 확인합니다.  

![Screen Shot 2020-04-30 at 3 37 34 PM](https://user-images.githubusercontent.com/6407492/80679798-8ec4b500-8af8-11ea-85e0-89f0eac0d8a7.png)  

### 3.4. ETL 작업 생성 및 실행  

* ETL 메뉴, Jobs로 이동해서 ETL Job을 생성합니다.  

<img width="1216" alt="Screen Shot 2020-04-30 at 2 06 41 PM" src="https://user-images.githubusercontent.com/6407492/80675204-de05e800-8aee-11ea-9ca7-0f94ee774563.png">

* 전 단계에서 사용했던 **AWSGlueServiceRole-test**를 설정합니다.  

<img width="1054" alt="Screen Shot 2020-04-30 at 2 07 26 PM" src="https://user-images.githubusercontent.com/6407492/80675206-df371500-8aee-11ea-9bf7-fb08f7b2c20f.png">

* Data source를 선택합니다.  

<img width="1183" alt="Screen Shot 2020-04-30 at 2 07 46 PM" src="https://user-images.githubusercontent.com/6407492/80675207-e1996f00-8aee-11ea-9415-901da8bbe160.png">

* Transform type을 Change schema로 선택합니다.  

<img width="899" alt="Screen Shot 2020-04-30 at 2 07 57 PM" src="https://user-images.githubusercontent.com/6407492/80675210-e2ca9c00-8aee-11ea-99ac-66923b4292c4.png">

* Data target의 Data store는 S3, 형식은 CSV, Target path는 여러분이 원하는 S3 bucket path를 입력합니다.  

<img width="978" alt="Screen Shot 2020-04-30 at 2 09 04 PM" src="https://user-images.githubusercontent.com/6407492/80675213-e4945f80-8aee-11ea-8821-732fb4ade134.png">

* Data source와 Data target 컬럼의 매핑 정보를 확인합니다.  

<img width="1221" alt="Screen Shot 2020-04-30 at 2 09 50 PM" src="https://user-images.githubusercontent.com/6407492/80675216-e6f6b980-8aee-11ea-9e6b-0f169e41ce0a.png">

* Glue ETL은 내부적으로 Spark Python 코드를 사용해서 ETL 작업을 수행합니다.  

<img width="1223" alt="Screen Shot 2020-04-30 at 2 10 16 PM" src="https://user-images.githubusercontent.com/6407492/80675219-e8c07d00-8aee-11ea-80f7-76522d5b7dde.png">

* 생성한 ETL Job을 실행합니다.  

<img width="1222" alt="Screen Shot 2020-04-30 at 2 10 41 PM" src="https://user-images.githubusercontent.com/6407492/80675224-eb22d700-8aee-11ea-91d9-004f9405957a.png">

* 인프라 Provisioning에 9분, Job 실행에 1분 정도 소요된 것을 확인할 수 있습니다. 

<img width="1150" alt="Screen Shot 2020-04-30 at 2 26 15 PM" src="https://user-images.githubusercontent.com/6407492/80675250-faa22000-8aee-11ea-86ad-81fef336d964.png">

* S3 bucket path에 CSV 형식으로 데이터가 추가되었는지 확인합니다.  

<img width="1273" alt="Screen Shot 2020-04-30 at 2 27 34 PM" src="https://user-images.githubusercontent.com/6407492/80675252-fbd34d00-8aee-11ea-9abb-444faf376335.png">

* 파일 1개를 선택해서 데이터를 확인합니다.  

<img width="794" alt="Screen Shot 2020-04-30 at 2 28 06 PM" src="https://user-images.githubusercontent.com/6407492/80675255-fd9d1080-8aee-11ea-9ffa-0f438cd38bb1.png">


---
### Table of Contents
[Analytics on AWS](README.md)
1. [Oracle RDS 생성](100.create_oracle_rds.md)
2. [Sample 테이블 생성 및 데이터 추가](200.load_sample_data.md)
3. [Glue를 통해 Oracle 데이터를 S3로 이동](300.migrate_data_through_glue.md)
4. [DMS를 통해 Oracle 데이터를 S3로 이동](400.migrate_data_through_dms.md)
5. [Glue를 통해 S3 데이터에 대한 데이터 카탈로그 구성](500.create_data_catalog.md)
6. [Redshift에서 S3 데이터 조회](600.select_s3_data_in_redshift.md)
7. [Dev Endpoint 사용](700.use_dev_endpoint.md)
