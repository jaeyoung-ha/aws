## 6. Redshift에서 S3 데이터 조회

### 6.1. Redshift 클러스터 구성

* Node type을 DC2로 해서 Redshift 클러스터를 구성합니다.

<img width="827" alt="Screen Shot 2020-04-30 at 10 13 02 PM" src="https://user-images.githubusercontent.com/6407492/80714502-fea26200-8b2f-11ea-8609-f9349865731e.png">

* 이번 실습에서는 Node 개수는 1개로 설정합니다.

<img width="822" alt="Screen Shot 2020-04-30 at 10 14 04 PM" src="https://user-images.githubusercontent.com/6407492/80714514-02ce7f80-8b30-11ea-8d81-b261edd52a36.png">

* Master user password를 설정하고, Redshift 클러스터를 생성합니다.

<img width="823" alt="Screen Shot 2020-04-30 at 10 14 31 PM" src="https://user-images.githubusercontent.com/6407492/80714558-0a8e2400-8b30-11ea-80cd-9c8a022b6877.png">

### 6.2. Spectrum 구성을 통한 S3 데이터 조회

* Spectrum 사용을 위한 **mySpectrumRole** 역할(Role)을 생성합니다.

<img width="1039" alt="Screen Shot 2020-04-30 at 10 21 30 PM" src="https://user-images.githubusercontent.com/6407492/80715432-54c3d500-8b31-11ea-91f0-8d5a8d93b196.png">

* Redshift - Customizable을 설정합니다.  

<img width="1031" alt="Screen Shot 2020-04-30 at 10 22 29 PM" src="https://user-images.githubusercontent.com/6407492/80715443-55f50200-8b31-11ea-9e74-7ad22dad049d.png">

* **AmazonS3ReadOnlyAccess**, **AWSGlueConsoleFullAccess** 정책(Policy)를 추가합니다.   

<img width="1085" alt="Screen Shot 2020-04-30 at 10 24 08 PM" src="https://user-images.githubusercontent.com/6407492/80715451-57bec580-8b31-11ea-954e-2cf436eacf8a.png">

* Redshift 클러스터에 역할을 추가합니다.  

<img width="1305" alt="Screen Shot 2020-04-30 at 10 29 16 PM" src="https://user-images.githubusercontent.com/6407492/80716113-3ad6c200-8b32-11ea-8be7-c9bf0f99c934.png">

* **mySpectrumRole** 역할을 선택하고 추가합니다.  

<img width="840" alt="Screen Shot 2020-04-30 at 10 30 06 PM" src="https://user-images.githubusercontent.com/6407492/80716386-88532f00-8b32-11ea-8915-ad0530c01d12.png"> 

* **Editor**를 통해서 Redshift에 연결합니다. 

<img width="843" alt="Screen Shot 2020-04-30 at 10 37 12 PM" src="https://user-images.githubusercontent.com/6407492/80716965-437bc800-8b33-11ea-8f6c-7a76363ff69b.png">

* External Schema를 생성합니다.

<img width="850" alt="Screen Shot 2020-04-30 at 10 42 04 PM" src="https://user-images.githubusercontent.com/6407492/80717407-dd437500-8b33-11ea-9820-64d449a34cde.png">

```sql
create external schema spectrum
from data catalog
database 'spectrumdb' 
iam_role 'arn:aws:iam::603755467080:role/mySpectrumRole'
create external database if not exists;
```
* External Table을 생성합니다.  

<img width="1035" alt="Screen Shot 2020-04-30 at 10 45 50 PM" src="https://user-images.githubusercontent.com/6407492/80717863-5c38ad80-8b34-11ea-81a1-d05295e3081d.png">

  * 여러분의 S3 Bucket Name으로 변경해야 합니다.  
  
```
create external table spectrum.emp (
empno integer,
ename varchar(10),
job varchar(9),
mgr integer,
hiredate date,
sal decimal(7,2),
comm decimal(7,2),
deptno integer)
row format delimited
fields terminated by ','
stored as textfile
location 's3://dms-target-test-bucket/csv/ADMIN/EMP/'
table properties ('numRows' = '20');
```

* External Table을 조회합니다. 

<img width="1046" alt="Screen Shot 2020-04-30 at 10 48 54 PM" src="https://user-images.githubusercontent.com/6407492/80718217-d23d1480-8b34-11ea-8962-5facbf65e109.png">

```sql
SELECT * FROM spectrum.Emp;
```

* 아래와 같이 S3에 있는 데이터를 조회할 수 있습니다.  

<img width="976" alt="Screen Shot 2020-04-30 at 10 49 08 PM" src="https://user-images.githubusercontent.com/6407492/80718223-d49f6e80-8b34-11ea-89fc-6993e1247d66.png">

* Redshift에서 Spectrum을 사용해서 S3 데이터에 대한 External Table을 생성하면, Glue의 Data catalog에 아래와 같이 자동으로 추가됩니다.  

<img width="1369" alt="Screen Shot 2020-04-30 at 10 54 04 PM" src="https://user-images.githubusercontent.com/6407492/80718749-82128200-8b35-11ea-80ca-de6cc5467277.png">

* spectrumdb의 emp를 선택하고, Action > View data를 클릭합니다.

<img width="1361" alt="Screen Shot 2020-04-30 at 10 55 58 PM" src="https://user-images.githubusercontent.com/6407492/80718998-d289df80-8b35-11ea-86ee-c1f9960fa961.png">

* Athena에서도 아래와 같이 데이터를 조회할 수 있습니다.  

<img width="1360" alt="Screen Shot 2020-04-30 at 10 56 20 PM" src="https://user-images.githubusercontent.com/6407492/80719003-d453a300-8b35-11ea-815d-ea194459541b.png">

```sql
SELECT * FROM "spectrumdb"."emp" limit 10;
```

* Athena에서 2개 이상의 테이블을 Join해서 조회할 수도 있습니다.  

<img width="1368" alt="Screen Shot 2020-04-30 at 11 10 51 PM" src="https://user-images.githubusercontent.com/6407492/80720441-dae31a00-8b37-11ea-8f3d-caf5445c3ced.png"> 

```sql
SELECT * FROM "spectrumdb"."emp" a, "s3"."s3emp" b WHERE a.empno = b.col0
```

### 6.3. Glue를 통해 S3 데이터를 Redshift로 이동

* Redshift을 위한 Connection을 생성합니다.  

![Screen Shot 2020-05-01 at 8 22 51 AM](https://user-images.githubusercontent.com/6407492/80768288-fa546400-8b84-11ea-88f0-17c49d0ff3d7.png)

* Redshift 연결 정보를 입력합니다.  

![Screen Shot 2020-05-01 at 8 23 54 AM](https://user-images.githubusercontent.com/6407492/80768356-296ad580-8b85-11ea-899d-0b7d14305bb6.png)

* 입력한 정보를 확인하고 Connection 생성을 완료합니다.  

![Screen Shot 2020-05-01 at 8 24 52 AM](https://user-images.githubusercontent.com/6407492/80768389-40112c80-8b85-11ea-8c5d-82d3c4b89334.png)

* 생성한 Connection을 테스트합니다. 

![Screen Shot 2020-05-01 at 8 26 02 AM](https://user-images.githubusercontent.com/6407492/80768548-a7c77780-8b85-11ea-927e-4e998ff46599.png)

![Screen Shot 2020-05-01 at 8 27 41 AM](https://user-images.githubusercontent.com/6407492/80768555-aa29d180-8b85-11ea-8db9-7ed54b49e5a3.png)

![Screen Shot 2020-05-01 at 8 27 28 AM](https://user-images.githubusercontent.com/6407492/80768558-ad24c200-8b85-11ea-818e-92980cc159ed.png)

* Redshift에서 새로운 Schema "glue"를 생성합니다. 

![Screen Shot 2020-05-01 at 9 46 15 AM](https://user-images.githubusercontent.com/6407492/80772176-ced77680-8b90-11ea-8148-cbae8bb81ea6.png)

```sql
CREATE SCHEMA glue AUTHORIZATION awsuser;
```

* 새로운 Schema "glue"에 "test" 테이블을 생성합니다.

![Screen Shot 2020-05-01 at 10 49 56 AM](https://user-images.githubusercontent.com/6407492/80775199-e8c98700-8b99-11ea-93af-22dd0a14b7f5.png)

```sql
CREATE TABLE dev.Glue.Test (c1 int);
```

---

* Redshift의 Meta data를 가져오기 위한 Crawler를 생성합니다. 

![Screen Shot 2020-05-01 at 8 36 14 AM](https://user-images.githubusercontent.com/6407492/80768985-d6921d80-8b86-11ea-91e2-6ae04eada7c0.png)

* Crawler source type을 "Data stores"로 설정합니다.   

![Screen Shot 2020-05-01 at 8 37 00 AM](https://user-images.githubusercontent.com/6407492/80769087-1953f580-8b87-11ea-9ac1-e3fc88969932.png)

* Data Store(**JDBC**), Connection(**redshift**), Include path(**dev/glue/%**)를 입력합니다.

![Screen Shot 2020-05-01 at 9 49 27 AM](https://user-images.githubusercontent.com/6407492/80772270-1b22b680-8b91-11ea-87eb-1741912713ee.png)

* 다른 데이터 스토어가 없으므로, "No"를 선택하고 넘어갑니다.  

![Screen Shot 2020-05-01 at 8 54 15 AM](https://user-images.githubusercontent.com/6407492/80769830-6fc23380-8b89-11ea-9b13-cfdd12757822.png)

* **AWSGlueServiceRole-test**를 선택합니다.

![Screen Shot 2020-05-01 at 8 56 09 AM](https://user-images.githubusercontent.com/6407492/80769932-b6b02900-8b89-11ea-93ee-a92a66960836.png)

* "Run on demand" 스케줄을 선택합니다.  

![Screen Shot 2020-05-01 at 8 56 23 AM](https://user-images.githubusercontent.com/6407492/80769939-b9ab1980-8b89-11ea-929b-158ddbec2b40.png)

* Crawler 결과를 저장할 Database를 추가합니다.

![Screen Shot 2020-05-01 at 8 58 17 AM](https://user-images.githubusercontent.com/6407492/80770009-f5de7a00-8b89-11ea-93a2-a2f98c10e3da.png)

* 입력 정보를 확인하고 Crawler 생성을 완료합니다.   

![Screen Shot 2020-05-01 at 8 59 28 AM](https://user-images.githubusercontent.com/6407492/80770073-1eff0a80-8b8a-11ea-8e2b-80e895969849.png)

* Crawler를 실행합니다.  

![Screen Shot 2020-05-01 at 9 00 35 AM](https://user-images.githubusercontent.com/6407492/80770102-4229ba00-8b8a-11ea-9b63-416a9ae4dfaa.png)

* "redshift" Database에 "rsdev_glue_test" Table이 추가된 것을 확인합니다.  

<img width="1210" alt="Screen Shot 2020-05-01 at 10 57 48 AM" src="https://user-images.githubusercontent.com/6407492/80775465-a8b6d400-8b9a-11ea-8762-73b1020a6464.png">

---

* S3 데이터를 Redshift로 옮기기 위해 ETL Job을 생성합니다.    

![Screen Shot 2020-05-01 at 8 30 58 AM](https://user-images.githubusercontent.com/6407492/80768719-1f95a200-8b86-11ea-9d69-df6f444be870.png)

* 인프라 Provisioning을 빠르게 하기 위해, Maximum capacity를 "2"로 변경합니다.  

<img width="1002" alt="Screen Shot 2020-05-01 at 11 19 16 AM" src="https://user-images.githubusercontent.com/6407492/80776581-adc95280-8b9d-11ea-9ede-6b62501797fe.png">

* Data source를 선택합니다.  

![Screen Shot 2020-05-01 at 8 32 44 AM](https://user-images.githubusercontent.com/6407492/80768808-61264d00-8b86-11ea-8e89-27822cf3ca88.png)

* Transform type을 "Change schema"로 설정합니다.

![Screen Shot 2020-05-01 at 8 34 02 AM](https://user-images.githubusercontent.com/6407492/80768866-887d1a00-8b86-11ea-8151-2c1a163f1e3c.png)

* Data target을 설정합니다. Database를 지정하면, Public 스키마에 새로운 테이블이 생성됩니다.  

<img width="958" alt="Screen Shot 2020-05-01 at 11 21 12 AM" src="https://user-images.githubusercontent.com/6407492/80777116-17962c00-8b9f-11ea-8641-b22e76f05c21.png">

* Source Columns와 Target Columns를 매핑하고, Job 생성을 완료합니다.  

![Screen Shot 2020-05-01 at 9 17 49 AM](https://user-images.githubusercontent.com/6407492/80770943-c41ae280-8b8c-11ea-8a11-0e5507fd7261.png)

* Job 스크립트를 확인합니다.

![Screen Shot 2020-05-01 at 9 19 31 AM](https://user-images.githubusercontent.com/6407492/80771048-1b20b780-8b8d-11ea-942e-0875197b26fb.png)

* Job을 실행합니다. 

![Screen Shot 2020-05-01 at 9 19 44 AM](https://user-images.githubusercontent.com/6407492/80771054-1d831180-8b8d-11ea-9c0b-e376ba24562e.png)

* Job 실행결과를 확인합니다.

<img width="1232" alt="Screen Shot 2020-05-01 at 11 30 53 AM" src="https://user-images.githubusercontent.com/6407492/80777300-b0c54280-8b9f-11ea-95e8-894d334be34f.png">

* Redshift의 Public 스키마에 데이터가 정상적으로 옮겨졌는지 확인합니다.  

![Screen Shot 2020-05-01 at 11 29 27 AM](https://user-images.githubusercontent.com/6407492/80777177-43b1ad00-8b9f-11ea-9f1e-bbd210647fc2.png)

```sql
SELECT * FROM dev.Public.S3emp;

```

---
### Table of Contents
[Analytics on AWS](README.md)
1. [Oracle RDS 생성](100.create_oracle_rds.md)
2. [Sample 테이블 생성 및 데이터 추가](200.load_sample_data.md)
3. [Glue를 통해 Oracle 데이터를 S3로 이동](300.migrate_data_through_glue.md)
4. [DMS를 통해 Oracle 데이터를 S3로 이동](400.migrate_data_through_dms.md)
5. [Glue를 통해 S3 데이터에 대한 데이터 카탈로그 구성](500.create_data_catalog.md)
6. [Redshift에서 S3 데이터 조회](600.select_s3_data_in_redshift.md)
7. [Dev Endpoint 사용](700.use_dev_endpoint.md)
